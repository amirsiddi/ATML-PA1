{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Generative Models - Investigating VAE vs. GAN Biases\n"
      ],
      "metadata": {
        "id": "dA0CosVMLGn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Drive"
      ],
      "metadata": {
        "id": "yOo5X8BCQkbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1LbcxwzK-0S",
        "outputId": "e9cc6172-87fe-4d0b-e3e2-62e9290b32b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# mount the google drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the CIFAR-10 data:"
      ],
      "metadata": {
        "id": "DLIeYUdKSqlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch torchvision\n",
        "\n",
        "import os, math, itertools, random\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = 32\n",
        "NC = 3  # channels\n",
        "DATA_ROOT = \"/content/drive/MyDrive/datasets\"\n",
        "\n",
        "# VAE usually uses inputs in [0,1]; DCGAN prefers [-1,1] (tanh generator).\n",
        "# We'll build two separate loaders so each model gets what it needs.\n",
        "tfm_vae = transforms.Compose([\n",
        "    transforms.ToTensor(),  # [0,1]\n",
        "])\n",
        "\n",
        "tfm_gan = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))  # [-1,1]\n",
        "])\n",
        "\n",
        "train_vae = torchvision.datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform=tfm_vae)\n",
        "train_gan = torchvision.datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform=tfm_gan)\n",
        "\n",
        "dl_vae = DataLoader(train_vae, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "dl_gan = DataLoader(train_gan, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/AI Masters/ATML/PA1-Task2\"\n",
        "samples_vae = os.path.join(save_path, \"samples_vae\")\n",
        "samples_gan = os.path.join(save_path, \"samples_gan\")\n",
        "\n",
        "os.makedirs(samples_vae, exist_ok=True)\n",
        "os.makedirs(samples_gan, exist_ok=True)"
      ],
      "metadata": {
        "id": "tD4EbEG9SsmO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Variational Autoencoder (VAE)\n",
        "\n",
        "Simple convolutional VAE with BCE reconstruction + KL term. Latent size = 128."
      ],
      "metadata": {
        "id": "6q9D1ROkqqRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, z_dim=128):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Encoder: 32x32 -> 16 -> 8 -> 4\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1), nn.ReLU(True),     # 16x16\n",
        "            nn.Conv2d(64,128,4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),  # 8x8\n",
        "            nn.Conv2d(128,256,4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(True), # 4x4\n",
        "        )\n",
        "        self.enc_out = nn.Flatten()\n",
        "        self.fc_mu = nn.Linear(256*4*4, z_dim)\n",
        "        self.fc_logvar = nn.Linear(256*4*4, z_dim)\n",
        "\n",
        "        # Decoder: 4 -> 8 -> 16 -> 32\n",
        "        self.fc_dec = nn.Linear(z_dim, 256*4*4)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(True), # 8x8\n",
        "            nn.ConvTranspose2d(128,64, 4,2,1), nn.BatchNorm2d(64),  nn.ReLU(True), # 16x16\n",
        "            nn.ConvTranspose2d(64, 3,   4,2,1), nn.Sigmoid() # 32x32, output in [0,1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.enc(x); h = self.enc_out(h)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = (0.5*logvar).exp()\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.fc_dec(z).view(-1,256,4,4)\n",
        "        return self.dec(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar\n",
        "\n",
        "def vae_loss(x, x_hat, mu, logvar, beta=1.0):\n",
        "    # recon with BCE (x, x_hat in [0,1]); reduction=sum for stable KL scaling\n",
        "    bce = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    # KL(N(mu, sigma) || N(0, I))\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return (bce + beta*kl), bce, kl\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_vae_samples(model, n=64, z_dim=128, step=0):\n",
        "    model.eval()\n",
        "    z = torch.randn(n, z_dim, device=DEVICE)\n",
        "    imgs = model.decode(z).cpu()\n",
        "    utils.save_image(imgs, f\"{samples_vae}/vae_{step:06d}.png\", nrow=8)\n",
        "\n",
        "# Train VAE\n",
        "vae = ConvVAE(z_dim=128).to(DEVICE)\n",
        "opt_vae = torch.optim.Adam(vae.parameters(), lr=2e-4)\n",
        "\n",
        "EPOCHS_VAE = 20 # 5 in the beginning for sanity check, 20 otherwise\n",
        "global_step = 0\n",
        "for epoch in range(1, EPOCHS_VAE+1):\n",
        "    vae.train()\n",
        "    pbar = tqdm(dl_vae, desc=f\"VAE Epoch {epoch}/{EPOCHS_VAE}\")\n",
        "    running = {\"loss\":0., \"bce\":0., \"kl\":0., \"n\":0}\n",
        "    for x,_ in pbar:\n",
        "        x = x.to(DEVICE)\n",
        "        x_hat, mu, logvar = vae(x)\n",
        "        loss, bce, kl = vae_loss(x, x_hat, mu, logvar, beta=1.0)\n",
        "\n",
        "        opt_vae.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt_vae.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        running[\"loss\"] += loss.item()\n",
        "        running[\"bce\"]  += bce.item()\n",
        "        running[\"kl\"]   += kl.item()\n",
        "        running[\"n\"]    += bs\n",
        "        pbar.set_postfix(loss=running[\"loss\"]/running[\"n\"])\n",
        "\n",
        "        if global_step % 500 == 0:\n",
        "            save_vae_samples(vae, n=64, z_dim=vae.z_dim, step=global_step)\n",
        "        global_step += 1\n",
        "\n",
        "    # epoch-end samples\n",
        "    save_vae_samples(vae, n=64, z_dim=vae.z_dim, step=global_step)\n",
        "    torch.save(vae.state_dict(), f\"{save_path}/snap_vae_epoch{epoch}.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF9s7NaDqzkk",
        "outputId": "4c1c7ce9-b922-4250-942e-5697b65456d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VAE Epoch 1/20: 100%|██████████| 391/391 [00:04<00:00, 96.61it/s, loss=1.97e+3]\n",
            "VAE Epoch 2/20: 100%|██████████| 391/391 [00:03<00:00, 99.28it/s, loss=1.87e+3]\n",
            "VAE Epoch 3/20: 100%|██████████| 391/391 [00:03<00:00, 98.14it/s, loss=1.84e+3]\n",
            "VAE Epoch 4/20: 100%|██████████| 391/391 [00:04<00:00, 79.89it/s, loss=1.84e+3]\n",
            "VAE Epoch 5/20: 100%|██████████| 391/391 [00:04<00:00, 92.90it/s, loss=1.83e+3] \n",
            "VAE Epoch 6/20: 100%|██████████| 391/391 [00:03<00:00, 97.89it/s, loss=1.83e+3]\n",
            "VAE Epoch 7/20: 100%|██████████| 391/391 [00:03<00:00, 99.82it/s, loss=1.83e+3]\n",
            "VAE Epoch 8/20: 100%|██████████| 391/391 [00:04<00:00, 96.94it/s, loss=1.83e+3]\n",
            "VAE Epoch 9/20: 100%|██████████| 391/391 [00:04<00:00, 96.01it/s, loss=1.83e+3]\n",
            "VAE Epoch 10/20: 100%|██████████| 391/391 [00:04<00:00, 93.03it/s, loss=1.82e+3] \n",
            "VAE Epoch 11/20: 100%|██████████| 391/391 [00:04<00:00, 96.47it/s, loss=1.82e+3] \n",
            "VAE Epoch 12/20: 100%|██████████| 391/391 [00:03<00:00, 99.63it/s, loss=1.82e+3] \n",
            "VAE Epoch 13/20: 100%|██████████| 391/391 [00:04<00:00, 90.22it/s, loss=1.82e+3]\n",
            "VAE Epoch 14/20: 100%|██████████| 391/391 [00:03<00:00, 99.53it/s, loss=1.82e+3]\n",
            "VAE Epoch 15/20: 100%|██████████| 391/391 [00:03<00:00, 98.33it/s, loss=1.82e+3]\n",
            "VAE Epoch 16/20: 100%|██████████| 391/391 [00:04<00:00, 95.23it/s, loss=1.82e+3] \n",
            "VAE Epoch 17/20: 100%|██████████| 391/391 [00:04<00:00, 97.58it/s, loss=1.82e+3]\n",
            "VAE Epoch 18/20: 100%|██████████| 391/391 [00:04<00:00, 94.17it/s, loss=1.82e+3]\n",
            "VAE Epoch 19/20: 100%|██████████| 391/391 [00:04<00:00, 95.75it/s, loss=1.82e+3] \n",
            "VAE Epoch 20/20: 100%|██████████| 391/391 [00:04<00:00, 95.21it/s, loss=1.82e+3] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE Reconstruction:"
      ],
      "metadata": {
        "id": "M2eIgVjJgJr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3) Restore your trained VAE (pick the checkpoint you want) ---\n",
        "vae = ConvVAE(z_dim=128).to(DEVICE)\n",
        "ckpt_path = os.path.join(save_path, \"snap_vae_epoch20.pt\")  # <-- change epoch if needed\n",
        "vae.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "vae.eval()\n",
        "\n",
        "# --- 4) Make a test loader (no normalization; VAE expects [0,1]) ---\n",
        "tfm_vae = transforms.Compose([transforms.ToTensor()])  # [0,1]\n",
        "testset = torchvision.datasets.CIFAR10(root=save_path, train=False, download=True, transform=tfm_vae)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --- 5) Take a random batch, reconstruct, and save side-by-side ---\n",
        "@torch.no_grad()\n",
        "def save_vae_recons(model, loader, n=4, out_name=\"vae_recon.png\"):\n",
        "    x, _ = next(iter(loader))      # random batch due to shuffle=True\n",
        "    x = x[:n].to(DEVICE)           # take first n images\n",
        "    x_hat, _, _ = model(x)         # encode -> sample z -> decode\n",
        "\n",
        "    # Stack originals over reconstructions for visual comparison\n",
        "    grid = torch.cat([x.cpu(), x_hat.cpu()], dim=0)  # first n originals, then n recons\n",
        "    utils.save_image(grid, os.path.join(samples_vae, out_name), nrow=n)\n",
        "    print(f\"Saved to: {os.path.join(samples_vae, out_name)}\")\n",
        "\n",
        "save_vae_recons(vae, testloader, n=4, out_name=\"vae_recon_test.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSgAU2wQgLpd",
        "outputId": "5303b32e-e0dd-4391-f579-ec7e97674b65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: /content/drive/MyDrive/AI Masters/ATML/PA1-Task2/samples_vae/vae_recon_test.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DCGAN (GAN)\n",
        "\n",
        "Classic DCGAN for 32×32 RGB. Uses tanh output and [-1,1] inputs."
      ],
      "metadata": {
        "id": "ZWfCiep-sFKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NZ = 128   # latent size\n",
        "NGF = 128  # generator feature maps\n",
        "NDF = 128  # discriminator feature maps\n",
        "LR = 2e-4\n",
        "BETA1 = 0.5\n",
        "BETA2 = 0.999\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=NZ, ngf=NGF, nc=NC):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, ngf*4, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf*4), nn.ReLU(True),   # 4x4\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf*2), nn.ReLU(True),   # 8x8\n",
        "            nn.ConvTranspose2d(ngf*2, ngf,   4, 2, 1, bias=False), nn.BatchNorm2d(ngf),   nn.ReLU(True),   # 16x16\n",
        "            nn.ConvTranspose2d(ngf,   nc,    4, 2, 1, bias=False), nn.Tanh()                                # 32x32\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ndf=NDF, nc=NC):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(nc,  ndf,   4, 2, 1, bias=False), nn.LeakyReLU(0.2, True),       # 16x16\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, True), # 8x8\n",
        "            nn.Conv2d(ndf*2, ndf*4,4, 2, 1, bias=False), nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, True), # 4x4\n",
        "            nn.Conv2d(ndf*4, 1,    4, 1, 0, bias=False) # 1x1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).view(-1)\n",
        "\n",
        "G = Generator().to(DEVICE)\n",
        "D = Discriminator().to(DEVICE)\n",
        "\n",
        "optG = torch.optim.Adam(G.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
        "optD = torch.optim.Adam(D.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_gan_samples(G, step=0, n=64):\n",
        "    G.eval()\n",
        "    z = torch.randn(n, NZ, 1, 1, device=DEVICE)\n",
        "    fake = G(z).cpu()  # in [-1,1]\n",
        "    # denormalize to [0,1] for saving\n",
        "    imgs = (fake + 1)/2\n",
        "    utils.save_image(imgs, f\"{samples_gan}/gan_{step:06d}.png\", nrow=8)\n",
        "\n",
        "EPOCHS_GAN = 50\n",
        "fixed_z = torch.randn(64, NZ, 1, 1, device=DEVICE)  # for consistent previews\n",
        "step = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS_GAN+1):\n",
        "    pbar = tqdm(dl_gan, desc=f\"GAN Epoch {epoch}/{EPOCHS_GAN}\")\n",
        "    for x,_ in pbar:\n",
        "        x = x.to(DEVICE)  # real in [-1,1]\n",
        "\n",
        "        # 1) Update D: maximize log D(x) + log(1 - D(G(z)))\n",
        "        D.train(); G.train()\n",
        "        bs = x.size(0)\n",
        "        real_labels = torch.ones(bs, device=DEVICE)\n",
        "        fake_labels = torch.zeros(bs, device=DEVICE)\n",
        "\n",
        "        # real\n",
        "        d_real = D(x)\n",
        "        loss_real = criterion(d_real, real_labels)\n",
        "\n",
        "        # fake\n",
        "        z = torch.randn(bs, NZ, 1, 1, device=DEVICE)\n",
        "        with torch.no_grad():\n",
        "            x_fake = G(z)\n",
        "        d_fake = D(x_fake)\n",
        "        loss_fake = criterion(d_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        optD.zero_grad(set_to_none=True)\n",
        "        loss_D.backward()\n",
        "        optD.step()\n",
        "\n",
        "        # 2) Update G: maximize log D(G(z))  <=> minimize BCE(D(G(z)), 1)\n",
        "        z = torch.randn(bs, NZ, 1, 1, device=DEVICE)\n",
        "        x_fake = G(z)\n",
        "        d_fake = D(x_fake)\n",
        "        loss_G = criterion(d_fake, real_labels)\n",
        "\n",
        "        optG.zero_grad(set_to_none=True)\n",
        "        loss_G.backward()\n",
        "        optG.step()\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            with torch.no_grad():\n",
        "                imgs = G(fixed_z).cpu()\n",
        "                utils.save_image((imgs+1)/2, f\"{samples_gan}/gan_fixed_{step:06d}.png\", nrow=8)\n",
        "        step += 1\n",
        "\n",
        "    save_gan_samples(G, step=step)\n",
        "    torch.save(G.state_dict(), f\"{save_path}/snap_g_epoch{epoch}.pt\")\n",
        "    torch.save(D.state_dict(), f\"{save_path}/snap_d_epoch{epoch}.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmgzA_0esI5V",
        "outputId": "bf89737b-5e4e-4a3f-9e2c-0f2020a2992e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GAN Epoch 1/50: 100%|██████████| 391/391 [00:06<00:00, 56.59it/s]\n",
            "GAN Epoch 2/50: 100%|██████████| 391/391 [00:06<00:00, 62.32it/s]\n",
            "GAN Epoch 3/50: 100%|██████████| 391/391 [00:06<00:00, 61.05it/s]\n",
            "GAN Epoch 4/50: 100%|██████████| 391/391 [00:06<00:00, 61.26it/s]\n",
            "GAN Epoch 5/50: 100%|██████████| 391/391 [00:06<00:00, 61.70it/s]\n",
            "GAN Epoch 6/50: 100%|██████████| 391/391 [00:06<00:00, 61.57it/s]\n",
            "GAN Epoch 7/50: 100%|██████████| 391/391 [00:06<00:00, 65.05it/s]\n",
            "GAN Epoch 8/50: 100%|██████████| 391/391 [00:06<00:00, 59.81it/s]\n",
            "GAN Epoch 9/50: 100%|██████████| 391/391 [00:06<00:00, 63.03it/s]\n",
            "GAN Epoch 10/50: 100%|██████████| 391/391 [00:06<00:00, 63.29it/s]\n",
            "GAN Epoch 11/50: 100%|██████████| 391/391 [00:06<00:00, 62.21it/s]\n",
            "GAN Epoch 12/50: 100%|██████████| 391/391 [00:06<00:00, 62.72it/s]\n",
            "GAN Epoch 13/50: 100%|██████████| 391/391 [00:06<00:00, 64.57it/s]\n",
            "GAN Epoch 14/50: 100%|██████████| 391/391 [00:06<00:00, 58.92it/s]\n",
            "GAN Epoch 15/50: 100%|██████████| 391/391 [00:06<00:00, 63.79it/s]\n",
            "GAN Epoch 16/50: 100%|██████████| 391/391 [00:06<00:00, 60.49it/s]\n",
            "GAN Epoch 17/50: 100%|██████████| 391/391 [00:06<00:00, 61.86it/s]\n",
            "GAN Epoch 18/50: 100%|██████████| 391/391 [00:06<00:00, 60.42it/s]\n",
            "GAN Epoch 19/50: 100%|██████████| 391/391 [00:06<00:00, 63.68it/s]\n",
            "GAN Epoch 20/50: 100%|██████████| 391/391 [00:06<00:00, 62.82it/s]\n",
            "GAN Epoch 21/50: 100%|██████████| 391/391 [00:06<00:00, 63.92it/s]\n",
            "GAN Epoch 22/50: 100%|██████████| 391/391 [00:06<00:00, 61.35it/s]\n",
            "GAN Epoch 23/50: 100%|██████████| 391/391 [00:06<00:00, 63.79it/s]\n",
            "GAN Epoch 24/50: 100%|██████████| 391/391 [00:06<00:00, 62.41it/s]\n",
            "GAN Epoch 25/50: 100%|██████████| 391/391 [00:06<00:00, 62.10it/s]\n",
            "GAN Epoch 26/50: 100%|██████████| 391/391 [00:06<00:00, 61.14it/s]\n",
            "GAN Epoch 27/50: 100%|██████████| 391/391 [00:06<00:00, 61.27it/s]\n",
            "GAN Epoch 28/50: 100%|██████████| 391/391 [00:06<00:00, 63.59it/s]\n",
            "GAN Epoch 29/50: 100%|██████████| 391/391 [00:06<00:00, 61.38it/s]\n",
            "GAN Epoch 30/50: 100%|██████████| 391/391 [00:06<00:00, 59.82it/s]\n",
            "GAN Epoch 31/50: 100%|██████████| 391/391 [00:06<00:00, 64.81it/s]\n",
            "GAN Epoch 32/50: 100%|██████████| 391/391 [00:06<00:00, 60.94it/s]\n",
            "GAN Epoch 33/50: 100%|██████████| 391/391 [00:06<00:00, 62.58it/s]\n",
            "GAN Epoch 34/50: 100%|██████████| 391/391 [00:06<00:00, 63.02it/s]\n",
            "GAN Epoch 35/50: 100%|██████████| 391/391 [00:06<00:00, 61.47it/s]\n",
            "GAN Epoch 36/50: 100%|██████████| 391/391 [00:06<00:00, 63.21it/s]\n",
            "GAN Epoch 37/50: 100%|██████████| 391/391 [00:06<00:00, 60.07it/s]\n",
            "GAN Epoch 38/50: 100%|██████████| 391/391 [00:06<00:00, 63.54it/s]\n",
            "GAN Epoch 39/50: 100%|██████████| 391/391 [00:06<00:00, 58.83it/s]\n",
            "GAN Epoch 40/50: 100%|██████████| 391/391 [00:06<00:00, 61.12it/s]\n",
            "GAN Epoch 41/50: 100%|██████████| 391/391 [00:06<00:00, 57.68it/s]\n",
            "GAN Epoch 42/50: 100%|██████████| 391/391 [00:06<00:00, 62.26it/s]\n",
            "GAN Epoch 43/50: 100%|██████████| 391/391 [00:06<00:00, 64.74it/s]\n",
            "GAN Epoch 44/50: 100%|██████████| 391/391 [00:06<00:00, 62.80it/s]\n",
            "GAN Epoch 45/50: 100%|██████████| 391/391 [00:06<00:00, 60.72it/s]\n",
            "GAN Epoch 46/50: 100%|██████████| 391/391 [00:05<00:00, 65.96it/s]\n",
            "GAN Epoch 47/50: 100%|██████████| 391/391 [00:06<00:00, 61.77it/s]\n",
            "GAN Epoch 48/50: 100%|██████████| 391/391 [00:06<00:00, 60.14it/s]\n",
            "GAN Epoch 49/50: 100%|██████████| 391/391 [00:06<00:00, 61.93it/s]\n",
            "GAN Epoch 50/50: 100%|██████████| 391/391 [00:06<00:00, 63.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Space Structure - helper code:"
      ],
      "metadata": {
        "id": "KhB1X9A90ph9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional, Iterable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import utils as vutils\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Globals and small utilities\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "try:\n",
        "    DEVICE  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "_CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "_CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "def get_cifar10_transforms(normalize: bool = True):\n",
        "    t = [transforms.ToTensor()]\n",
        "    if normalize:\n",
        "        t.append(transforms.Normalize(_CIFAR10_MEAN, _CIFAR10_STD))\n",
        "    return transforms.Compose(t)\n",
        "\n",
        "def denormalize_cifar10(x: Tensor) -> Tensor:\n",
        "    \"\"\"Inverse of CIFAR-10 normalization. Clamps into [0,1].\"\"\"\n",
        "    mean = torch.tensor(_CIFAR10_MEAN, device=x.device).view(1, -1, 1, 1)\n",
        "    std  = torch.tensor(_CIFAR10_STD,  device=x.device).view(1, -1, 1, 1)\n",
        "    x = x * std + mean\n",
        "    return x.clamp(0, 1)\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def to_device(x: Tensor) -> Tensor:\n",
        "    return x.to(DEVICE, non_blocking=True)\n",
        "\n",
        "def reparameterize(mu: Tensor, logvar: Tensor, deterministic: bool = False) -> Tensor:\n",
        "    if deterministic:\n",
        "        return mu\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps * std\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Picking two test images of different classes (CIFAR-10)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def pick_two_test_images_from_different_classes(\n",
        "    root: str = DATA_ROOT,\n",
        "\n",
        "    normalize: bool = True,\n",
        "    seed: Optional[int] = 123\n",
        ") -> Tuple[Tuple[Tensor, int], Tuple[Tensor, int]]:\n",
        "    \"\"\"\n",
        "    Returns two (image, label) tuples from CIFAR-10 test set with different labels.\n",
        "    Images are returned as CHW tensors, optionally normalized with CIFAR-10 stats.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    tfm = get_cifar10_transforms(normalize=normalize)\n",
        "    testset = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
        "\n",
        "    idx1 = random.randrange(len(testset))\n",
        "    x1, y1 = testset[idx1]\n",
        "\n",
        "    # Find another with a different class\n",
        "    for _ in range(20000):\n",
        "        idx2 = random.randrange(len(testset))\n",
        "        x2, y2 = testset[idx2]\n",
        "        if y2 != y1:\n",
        "            return (x1, y1), (x2, y2)\n",
        "\n",
        "    # fallback (shouldn't happen)\n",
        "    return (x1, y1), (x2, y2)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Interpolation math (LERP and SLERP)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def lerp(z1: Tensor, z2: Tensor, t: float) -> Tensor:\n",
        "    return (1.0 - t) * z1 + t * z2\n",
        "\n",
        "def slerp(z1: Tensor, z2: Tensor, t: float, eps: float = 1e-7) -> Tensor:\n",
        "    \"\"\"\n",
        "    Spherical linear interpolation for smoother perceptual paths in GAN latent spaces.\n",
        "    Works for z vectors (B, Z) or (Z,). Operates per-sample if batched.\n",
        "    \"\"\"\n",
        "    # Flatten batch if needed\n",
        "    squeeze = False\n",
        "    if z1.dim() == 1:\n",
        "        z1 = z1.unsqueeze(0)\n",
        "        z2 = z2.unsqueeze(0)\n",
        "        squeeze = True\n",
        "\n",
        "    z1n = torch.nn.functional.normalize(z1, dim=1, eps=eps)\n",
        "    z2n = torch.nn.functional.normalize(z2, dim=1, eps=eps)\n",
        "    dot = (z1n * z2n).sum(dim=1, keepdim=True).clamp(-1 + eps, 1 - eps)\n",
        "    omega = torch.acos(dot)\n",
        "    so = torch.sin(omega)\n",
        "\n",
        "    t = torch.as_tensor(t, device=z1.device, dtype=z1.dtype).view(1, 1)\n",
        "\n",
        "    # When z1 ~= z2, fall back to lerp to avoid division by small numbers\n",
        "    mask = (so < 1e-3).float()\n",
        "    s1 = torch.sin((1.0 - t) * omega) / (so + eps)\n",
        "    s2 = torch.sin(t * omega) / (so + eps)\n",
        "    out = (s1 * z1 + s2 * z2) * (1.0 - mask) + lerp(z1, z2, t.item()) * mask\n",
        "\n",
        "    if squeeze:\n",
        "        out = out.squeeze(0)\n",
        "    return out\n",
        "\n",
        "def interp_1d(z1: Tensor, z2: Tensor, n_intermediate: int, mode: str = 'lerp') -> Tensor:\n",
        "    \"\"\"\n",
        "    Returns a tensor of shape (n_intermediate+2, Z) including endpoints.\n",
        "    \"\"\"\n",
        "    ts = torch.linspace(0.0, 1.0, steps=n_intermediate + 2, device=z1.device, dtype=z1.dtype)\n",
        "    outs = []\n",
        "    for t in ts.tolist():\n",
        "        if mode == 'slerp':\n",
        "            outs.append(slerp(z1, z2, t))\n",
        "        else:\n",
        "            outs.append(lerp(z1, z2, t))\n",
        "    return torch.stack(outs, dim=0)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# VAE interpolation\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def interpolate_vae_and_save_grid(\n",
        "    model,\n",
        "    x1: Tensor, x2: Tensor,\n",
        "    n_intermediate: int = 10,\n",
        "    out_path: str = f'{save_path}/samples_vae/interp/vae_interp_grid.png',\n",
        "    use_mu: bool = True,\n",
        "    detach: bool = True,\n",
        "    denormalize_before_save: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    - Encodes x1, x2 with model.encode -> (mu, logvar)\n",
        "    - Takes z = mu (default) for each endpoint, optionally samples via reparameterize\n",
        "    - Linearly interpolates between z1 and z2\n",
        "    - Decodes each z to an image and saves a single-row grid\n",
        "    \"\"\"\n",
        "    ensure_dir(out_path)\n",
        "    model.eval()\n",
        "\n",
        "    x1 = x1.unsqueeze(0).to(DEVICE)\n",
        "    x2 = x2.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # Some VAEs implement encode(x) -> (mu, logvar); support both possibilities.\n",
        "    enc1 = model.encode(x1)\n",
        "    enc2 = model.encode(x2)\n",
        "\n",
        "    if isinstance(enc1, (tuple, list)) and len(enc1) >= 2:\n",
        "        mu1, logvar1 = enc1[0], enc1[1]\n",
        "    else:\n",
        "        raise ValueError(\"model.encode(x) must return (mu, logvar) for this helper.\")\n",
        "\n",
        "    if isinstance(enc2, (tuple, list)) and len(enc2) >= 2:\n",
        "        mu2, logvar2 = enc2[0], enc2[1]\n",
        "    else:\n",
        "        raise ValueError(\"model.encode(x) must return (mu, logvar) for this helper.\")\n",
        "\n",
        "    z1 = reparameterize(mu1, logvar1, deterministic=use_mu).squeeze(0)\n",
        "    z2 = reparameterize(mu2, logvar2, deterministic=use_mu).squeeze(0)\n",
        "\n",
        "    zs = interp_1d(z1, z2, n_intermediate=n_intermediate, mode='lerp')  # VAE: lerp is typical\n",
        "    imgs = model.decode(zs.to(DEVICE))\n",
        "\n",
        "    if imgs.dim() != 4:\n",
        "        raise ValueError(\"Decoded images must be a 4D tensor (N,C,H,W).\")\n",
        "\n",
        "    # VAE usually outputs [0,1]; clamp to be safe\n",
        "    grid = vutils.make_grid(imgs.clamp(0, 1), nrow=imgs.size(0), padding=2)\n",
        "    vutils.save_image(grid, out_path)\n",
        "    return out_path\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# GAN interpolation\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def interpolate_gan_and_save_grid(G, z1: Optional[Tensor]=None, z2: Optional[Tensor]=None, z_dim: int=128,\n",
        "                                  n_intermediate: int=10, out_path: str='samples_gan/interp/gan_interp_grid.png',\n",
        "                                  mode: str='slerp', tanh_output: bool=True, reshape_policy: str='auto'):\n",
        "    ensure_dir(out_path); G.eval()\n",
        "    if z1 is None: z1=torch.randn(z_dim, device=DEVICE)\n",
        "    if z2 is None: z2=torch.randn(z_dim, device=DEVICE)\n",
        "    zs = interp_1d(z1, z2, n_intermediate, mode=mode).to(DEVICE)  # (T,Z)\n",
        "\n",
        "    def forward(zflat: Tensor):\n",
        "        if reshape_policy=='always':\n",
        "            return G(zflat.view(zflat.size(0), zflat.size(1), 1, 1))\n",
        "        if reshape_policy=='never':\n",
        "            return G(zflat)\n",
        "        # auto\n",
        "        try:\n",
        "            return G(zflat)\n",
        "        except RuntimeError:\n",
        "            return G(zflat.view(zflat.size(0), zflat.size(1), 1, 1))\n",
        "\n",
        "    imgs = forward(zs)\n",
        "    if imgs.dim()!=4: raise ValueError(\"Generator must return (N,C,H,W).\")\n",
        "    if tanh_output: imgs = (imgs+1)/2.0\n",
        "    imgs = imgs.clamp(0,1)\n",
        "    vutils.save_image(vutils.make_grid(imgs, nrow=imgs.size(0), padding=2), out_path)\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "R0CXD2fT0r4d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual Code for Interpolation:"
      ],
      "metadata": {
        "id": "y85anxX76JYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Two CIFAR-10 test images from different classes (normalized for your encoder)\n",
        "(x1, y1), (x2, y2) = pick_two_test_images_from_different_classes(root='./data', normalize=True)\n",
        "\n",
        "# 2) VAE interpolation (μ-only for clean path). Saves a 12-frame row: endpoints + 10 steps\n",
        "vae_grid_path = interpolate_vae_and_save_grid(\n",
        "    model=vae,        # <-- your VAE instance\n",
        "    x1=x1, x2=x2,\n",
        "    n_intermediate=10,\n",
        "    out_path=f'{save_path}/samples_vae/interp/vae_interp_grid.png',\n",
        "    use_mu=True,            # set False to sample via reparameterization\n",
        ")\n",
        "\n",
        "# 3) GAN interpolation (SLERP recommended)\n",
        "gan_grid_path = interpolate_gan_and_save_grid(\n",
        "    G=G,                    # <-- your trained generator\n",
        "    z_dim=128,              # match your training\n",
        "    n_intermediate=10,\n",
        "    out_path=f'{save_path}/samples_gan/interp/gan_interp_grid.png',\n",
        "    mode='slerp',           # 'lerp' also available\n",
        "    tanh_output=True        # set False if G outputs [0,1]\n",
        ")\n",
        "\n",
        "print(\"Saved:\", vae_grid_path, gan_grid_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KARRWp4k6Fri",
        "outputId": "2069fa9c-21ae-4723-9b9a-34d74387bc41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.05MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/AI Masters/ATML/PA1-Task2/samples_vae/interp/vae_interp_grid.png /content/drive/MyDrive/AI Masters/ATML/PA1-Task2/samples_gan/interp/gan_interp_grid.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions for 2D PCA."
      ],
      "metadata": {
        "id": "Ml2VkpDJLPc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plot_vae_pca.py\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from sklearn.decomposition import PCA as SKPCA\n",
        "    _HAVE_SKLEARN = True\n",
        "except Exception:\n",
        "    _HAVE_SKLEARN = False\n",
        "\n",
        "try:\n",
        "    DEVICE  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "_CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "_CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
        "_CIFAR10_CLASSES = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "def get_cifar10_transform(normalize: bool=True):\n",
        "    tfms = [transforms.ToTensor()]\n",
        "    if normalize:\n",
        "        tfms.append(transforms.Normalize(_CIFAR10_MEAN, _CIFAR10_STD))\n",
        "    return transforms.Compose(tfms)\n",
        "\n",
        "def collect_10_per_class(root: str=DATA_ROOT, normalize: bool=True):\n",
        "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=get_cifar10_transform(normalize))\n",
        "    per_class = {k: [] for k in range(10)}\n",
        "    for idx in range(len(ds)):\n",
        "        x, y = ds[idx]\n",
        "        if len(per_class[y]) < 10:\n",
        "            per_class[y].append(x)\n",
        "        if all(len(v) >= 10 for v in per_class.values()):\n",
        "            break\n",
        "    xs, ys = [], []\n",
        "    for c in range(10):\n",
        "        xs.extend(per_class[c])\n",
        "        ys.extend([c]*len(per_class[c]))\n",
        "    X = torch.stack(xs, dim=0)  # [100,3,32,32]\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return X, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_vae_mu(model, X: Tensor, batch_size: int=64) -> Tensor:\n",
        "    model.eval()\n",
        "    Zs = []\n",
        "    for i in range(0, X.size(0), batch_size):\n",
        "        xb = X[i:i+batch_size].to(DEVICE)\n",
        "        mu, logvar = model.encode(xb)[:2]\n",
        "        Zs.append(mu.detach().cpu())\n",
        "    return torch.cat(Zs, dim=0)\n",
        "\n",
        "def pca_2d(Z: Tensor) -> Tensor:\n",
        "    Z = Z.detach().cpu().float()\n",
        "    if _HAVE_SKLEARN:\n",
        "        pca = SKPCA(n_components=2, svd_solver='auto', random_state=42)\n",
        "        import numpy as np\n",
        "        Z2 = torch.from_numpy(pca.fit_transform(Z.numpy()))\n",
        "    else:\n",
        "        Zc = Z - Z.mean(dim=0, keepdim=True)\n",
        "        U, S, Vh = torch.linalg.svd(Zc, full_matrices=False)\n",
        "        comps = Vh[:2]  # [2, D]\n",
        "        Z2 = Zc @ comps.t()\n",
        "    return Z2\n",
        "\n",
        "def plot_scatter_by_class(Z2: Tensor, y: Tensor, out_path: str) -> str:\n",
        "    plt.figure(figsize=(7.5, 6))\n",
        "    cmap = plt.get_cmap('tab10')\n",
        "    for c in range(10):\n",
        "        idx = (y==c).nonzero(as_tuple=False).squeeze(1)\n",
        "        pts = Z2[idx]\n",
        "        plt.scatter(pts[:,0].numpy(), pts[:,1].numpy(),\n",
        "                    label=_CIFAR10_CLASSES[c], s=28, alpha=0.85, c=[cmap(c)])\n",
        "    plt.legend(loc='best', fontsize=9, ncol=2, frameon=True)\n",
        "    plt.title('VAE Latent Space (PCA to 2D) — 10 images per CIFAR-10 class')\n",
        "    plt.xlabel('PC1'); plt.ylabel('PC2')\n",
        "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=160)\n",
        "    plt.close()\n",
        "    return out_path\n",
        "\n",
        "def plot_vae_latents_pca(model,\n",
        "                         root: str='./data',\n",
        "                         batch_size: int=64,\n",
        "                         normalize: bool=True,\n",
        "                         out_path: str='plots/vae_cifar10_pca.png') -> str:\n",
        "    X, y = collect_10_per_class(root=DATA_ROOT, normalize=normalize)\n",
        "    Z = encode_vae_mu(model, X, batch_size=batch_size)\n",
        "    Z2 = pca_2d(Z)\n",
        "    return plot_scatter_by_class(Z2, y, out_path)\n"
      ],
      "metadata": {
        "id": "KLSNxyKvLkdM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting 2D PCA:"
      ],
      "metadata": {
        "id": "AuABbMhETrPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig_path = plot_vae_latents_pca(\n",
        "    model=vae,\n",
        "    root=DATA_ROOT,\n",
        "    batch_size=64,\n",
        "    normalize=True,\n",
        "    out_path=f'{save_path}/plots/vae_cifar10_pca.png'\n",
        ")\n",
        "print(\"PCA plot saved:\", fig_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2SlMfJdTvs_",
        "outputId": "9b37565c-0ffa-4044-c88c-edf2f0b965fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA plot saved: /content/drive/MyDrive/AI Masters/ATML/PA1-Task2/plots/vae_cifar10_pca.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions for Semantic Factors:"
      ],
      "metadata": {
        "id": "TedgnRkTXQxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math, numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    DEVICE  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "_CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465]).view(1,3,1,1)\n",
        "_CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616]).view(1,3,1,1)\n",
        "_CIFAR10_NAMES = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "_CAT_LABEL = 3\n",
        "\n",
        "def _to_chw_float(x: np.ndarray) -> torch.Tensor:\n",
        "    if x.dtype != np.float32:\n",
        "        x = x.astype(np.float32) / 255.0\n",
        "    t = torch.from_numpy(x).permute(2,0,1).unsqueeze(0)  # 1x3x32x32\n",
        "    return t\n",
        "\n",
        "def _normalize(x: Tensor, normalize: bool=True) -> Tensor:\n",
        "    if not normalize: return x\n",
        "    return (x - _CIFAR10_MEAN) / _CIFAR10_STD\n",
        "\n",
        "def _load_cifar10_cat_keras(normalize: bool=True) -> Tuple[Tensor, int]:\n",
        "    from tensorflow.keras.datasets import cifar10\n",
        "    (_, _), (x_test, y_test) = cifar10.load_data()\n",
        "    y = y_test.flatten()\n",
        "    idx = int(np.where(y == _CAT_LABEL)[0][0])\n",
        "    img = x_test[idx]\n",
        "    x = _to_chw_float(img)\n",
        "    x = _normalize(x, normalize)\n",
        "    return x, int(_CAT_LABEL)\n",
        "\n",
        "def _ensure_dir(path: str):\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _make_single_row_grid(imgs: Tensor) -> np.ndarray:\n",
        "    N, C, H, W = imgs.shape\n",
        "    canvas = torch.zeros(C, H, W*N, dtype=imgs.dtype)\n",
        "    for i in range(N):\n",
        "        canvas[:, :, i*W:(i+1)*W] = imgs[i]\n",
        "    arr = canvas.permute(1,2,0).detach().cpu().numpy()\n",
        "    arr = np.clip(arr, 0.0, 1.0)\n",
        "    return arr\n",
        "\n",
        "@torch.no_grad()\n",
        "def vary_single_latent_axis_for_cat(\n",
        "    vae_model,\n",
        "    axis: Optional[int] = None,\n",
        "    steps: int = 11,\n",
        "    sigma_span: float = 3.0,\n",
        "    normalize_input: bool = True,\n",
        "    decoder_output: str = \"sigmoid\",   # 'sigmoid' or 'tanh'\n",
        "    out_path: str = f\"{save_path}/plots/vae_cat_axis_sweep.png\",\n",
        ") -> Tuple[str, List[float], int]:\n",
        "    vae_model.eval().to(DEVICE)\n",
        "\n",
        "    x, y = _load_cifar10_cat_keras(normalize=normalize_input)  # [1,3,32,32]\n",
        "    x = x.to(DEVICE)\n",
        "\n",
        "    enc = vae_model.encode(x)\n",
        "    if isinstance(enc, (tuple, list)) and len(enc) >= 2:\n",
        "        mu, logvar = enc[0], enc[1]\n",
        "    else:\n",
        "        raise ValueError(\"Expected vae_model.encode(x) -> (mu, logvar).\")\n",
        "\n",
        "    mu = mu.squeeze(0); logvar = logvar.squeeze(0)\n",
        "    sigma = torch.exp(0.5 * logvar)\n",
        "\n",
        "    if axis is None:\n",
        "        axis = int(torch.argmax(sigma).item())\n",
        "\n",
        "    vals = torch.linspace(-sigma_span, sigma_span, steps=steps, device=DEVICE)\n",
        "    values_used = (mu[axis] + vals * sigma[axis]).detach().cpu().tolist()\n",
        "\n",
        "    zs = []\n",
        "    for t in vals:\n",
        "        z = mu.clone()\n",
        "        z[axis] = mu[axis] + t * sigma[axis]\n",
        "        zs.append(z)\n",
        "    Z = torch.stack(zs, dim=0)\n",
        "\n",
        "    imgs = vae_model.decode(Z)\n",
        "    if decoder_output == \"tanh\":\n",
        "        imgs = (imgs + 1.0) / 2.0\n",
        "    imgs = imgs.clamp(0,1)\n",
        "\n",
        "    grid = _make_single_row_grid(imgs)\n",
        "    _ensure_dir(out_path)\n",
        "    plt.imsave(out_path, grid)\n",
        "    return out_path, values_used, axis\n"
      ],
      "metadata": {
        "id": "qYXJynNlXW7n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Factors Code: Changing one dimention while fixing all others."
      ],
      "metadata": {
        "id": "p7fhCNSfXtHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run traversal (auto-picks axis with largest posterior sigma)\n",
        "out_path, values_used, axis = vary_single_latent_axis_for_cat(\n",
        "    vae,\n",
        "    axis=None,            # or specify an int index, e.g., axis=7\n",
        "    steps=11,             # number of images (endpoints included)\n",
        "    sigma_span=3.0,       # sweep mu_k ± 3 * sigma_k\n",
        "    normalize_input=True, # set False if your VAE expects raw [0,1]\n",
        "    decoder_output=\"sigmoid\",  # \"tanh\" if decoder outputs [-1,1]\n",
        "    out_path=f\"{save_path}/plots/vae_cat_axis_sweep.png\"\n",
        ")\n",
        "print(\"Saved grid at:\", out_path)\n",
        "print(\"Axis varied:\", axis)\n",
        "print(\"Values used (z_k):\", values_used)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlblUD4EX0VV",
        "outputId": "45f82355-9036-4f1e-8efc-0447319141ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 1us/step\n",
            "Saved grid at: /content/drive/MyDrive/AI Masters/ATML/PA1-Task2/plots/vae_cat_axis_sweep.png\n",
            "Axis varied: 56\n",
            "Values used (z_k): [-6.496976852416992, -4.946293830871582, -3.3956100940704346, -1.8449268341064453, -0.29424333572387695, 1.2564395666122437, 2.8071231842041016, 4.35780668258667, 5.908490180969238, 7.459174156188965, 9.009857177734375]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper code for OOD reconstruction:"
      ],
      "metadata": {
        "id": "zA36qPHrbIfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    DEVICE  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# CIFAR-10 normalization constants\n",
        "_CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465]).view(1,3,1,1)\n",
        "_CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616]).view(1,3,1,1)\n",
        "\n",
        "# -------------------------------\n",
        "# Data helpers (no torchvision)\n",
        "# -------------------------------\n",
        "\n",
        "def _to_chw_float(x: np.ndarray) -> torch.Tensor:\n",
        "    # x: HxWxC uint8 or float in [0,1]; return 1x3x32x32 float32 in [0,1]\n",
        "    if x.dtype != np.float32:\n",
        "        x = x.astype(np.float32) / 255.0\n",
        "    t = torch.from_numpy(x).permute(2,0,1).unsqueeze(0)  # 1x3x32x32\n",
        "    return t\n",
        "\n",
        "def _normalize(x: torch.Tensor, normalize: bool=True) -> torch.Tensor:\n",
        "    if not normalize:\n",
        "        return x\n",
        "    return (x - _CIFAR10_MEAN) / _CIFAR10_STD\n",
        "\n",
        "def load_cifar10_cat_keras(normalize: bool=True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Load one 'cat' image from CIFAR-10 test set via Keras and return [1,3,32,32] tensor.\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.datasets import cifar10\n",
        "    (_, _), (x_test, y_test) = cifar10.load_data()\n",
        "    y = y_test.flatten()\n",
        "    idx = int(np.where(y == 3)[0][0])  # label 3 = 'cat'\n",
        "    x = _to_chw_float(x_test[idx])\n",
        "    x = _normalize(x, normalize)\n",
        "    return x\n",
        "\n",
        "def make_synthetic_house(normalize: bool=True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create a simple 32x32 'house' icon (square + triangle roof), [1,3,32,32] in [0,1] then normalize.\n",
        "    \"\"\"\n",
        "    H, W = 32, 32\n",
        "    img = np.ones((H,W,3), dtype=np.float32)  # white background\n",
        "\n",
        "    # House body (rectangle)\n",
        "    body_top, body_bottom = 14, 28\n",
        "    body_left, body_right = 8, 24\n",
        "    img[body_top:body_bottom, body_left:body_right, :] = np.array([0.6, 0.6, 0.8], dtype=np.float32)  # light bluish\n",
        "\n",
        "    # Door\n",
        "    door_top, door_bottom = 20, 28\n",
        "    door_left, door_right = 14, 18\n",
        "    img[door_top:door_bottom, door_left:door_right, :] = np.array([0.4, 0.2, 0.1], dtype=np.float32)\n",
        "\n",
        "    # Roof (triangle)\n",
        "    for r in range(10, 15):\n",
        "        span = r - 10  # increases with row\n",
        "        c_center = 16\n",
        "        c_left = c_center - span - 1\n",
        "        c_right = c_center + span + 1\n",
        "        img[r, max(0,c_left):min(W, c_right), :] = np.array([0.8, 0.2, 0.2], dtype=np.float32)\n",
        "\n",
        "    x = _to_chw_float((img * 255).astype(np.uint8))\n",
        "    x = _normalize(x, normalize)\n",
        "    return x\n",
        "\n",
        "# -------------------------------\n",
        "# VAE reconstruction + metrics\n",
        "# -------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def vae_reconstruct(vae_model, x: torch.Tensor, decoder_output: str=\"sigmoid\"):\n",
        "    \"\"\"\n",
        "    Given x [1,3,32,32], returns reconstruction x_hat [1,3,32,32] in [0,1] (mapped if decoder_output='tanh').\n",
        "    Uses z = mu for deterministic reconstruction.\n",
        "    \"\"\"\n",
        "    vae_model.eval().to(DEVICE)\n",
        "    x = x.to(DEVICE)\n",
        "\n",
        "    enc = vae_model.encode(x)\n",
        "    if isinstance(enc, (tuple, list)) and len(enc) >= 2:\n",
        "        mu, logvar = enc[0], enc[1]\n",
        "    else:\n",
        "        raise ValueError(\"Expected vae_model.encode(x) -> (mu, logvar).\")\n",
        "\n",
        "    z = mu  # deterministic\n",
        "    x_hat = vae_model.decode(z)\n",
        "\n",
        "    if decoder_output == \"tanh\":\n",
        "        x_hat = (x_hat + 1.0) / 2.0\n",
        "\n",
        "    return x_hat.clamp(0,1)\n",
        "\n",
        "def mse(a: torch.Tensor, b: torch.Tensor) -> float:\n",
        "    return float(torch.mean((a - b) ** 2).cpu())\n",
        "\n",
        "def l1(a: torch.Tensor, b: torch.Tensor) -> float:\n",
        "    return float(torch.mean(torch.abs(a - b)).cpu())\n",
        "\n",
        "def psnr(a: torch.Tensor, b: torch.Tensor, data_range: float = 1.0) -> float:\n",
        "    e = torch.mean((a - b) ** 2)\n",
        "    e = float(e.cpu())\n",
        "    if e == 0:\n",
        "        return float(\"inf\")\n",
        "    import math\n",
        "    return 20.0 * math.log10(data_range) - 10.0 * math.log10(e)\n",
        "\n",
        "def save_pair(original: torch.Tensor, recon: torch.Tensor, path: str, title_top: str, title_bottom: str):\n",
        "    \"\"\"\n",
        "    Save a 2-row image: original on top, reconstruction on bottom.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    orig = original.squeeze(0).permute(1,2,0).detach().cpu().numpy()\n",
        "    rec  = recon.squeeze(0).permute(1,2,0).detach().cpu().numpy()\n",
        "\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.figure(figsize=(3.2, 6.0))\n",
        "    plt.subplot(2,1,1); plt.imshow(orig); plt.axis(\"off\"); plt.title(title_top)\n",
        "    plt.subplot(2,1,2); plt.imshow(rec);  plt.axis(\"off\"); plt.title(title_bottom)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=140)\n",
        "    plt.close()\n",
        "\n",
        "def compare_cat_vs_house_reconstruction(\n",
        "    vae_model,\n",
        "    normalize_input: bool = True,\n",
        "    decoder_output: str = \"sigmoid\",\n",
        "    out_dir: str = \"plots/vae_compare\"\n",
        ") -> Tuple[Dict[str, float], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Reconstructs a CIFAR-10 'cat' sample and a synthetic 'house' image with the VAE,\n",
        "    computes MSE/L1/PSNR errors, and saves side-by-side images.\n",
        "    Returns (cat_metrics, house_metrics).\n",
        "    \"\"\"\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1) Get images\n",
        "    x_cat   = load_cifar10_cat_keras(normalize=normalize_input)     # [1,3,32,32]\n",
        "    x_house = make_synthetic_house(normalize=normalize_input)       # [1,3,32,32]\n",
        "\n",
        "    # 2) Reconstruct\n",
        "    x_cat_hat   = vae_reconstruct(vae_model, x_cat,   decoder_output=decoder_output)\n",
        "    x_house_hat = vae_reconstruct(vae_model, x_house, decoder_output=decoder_output)\n",
        "\n",
        "    # 3) Metrics\n",
        "    cat_metrics = {\n",
        "        \"MSE\":  mse(x_cat,   x_cat_hat),\n",
        "        \"L1\":   l1(x_cat,    x_cat_hat),\n",
        "        \"PSNR\": psnr(x_cat,  x_cat_hat),\n",
        "    }\n",
        "    house_metrics = {\n",
        "        \"MSE\":  mse(x_house,   x_house_hat),\n",
        "        \"L1\":   l1(x_house,    x_house_hat),\n",
        "        \"PSNR\": psnr(x_house,  x_house_hat),\n",
        "    }\n",
        "\n",
        "    # 4) Save visuals\n",
        "    save_pair(x_cat,   x_cat_hat,   f\"{out_dir}/cat_reconstruction.png\",   \"Original: Cat\",   \"Reconstruction: Cat\")\n",
        "    save_pair(x_house, x_house_hat, f\"{out_dir}/house_reconstruction.png\", \"Original: House\", \"Reconstruction: House\")\n",
        "\n",
        "    # 5) CSV\n",
        "    import csv\n",
        "    with open(f\"{out_dir}/reconstruction_metrics.csv\", \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"image\", \"MSE\", \"L1\", \"PSNR\"])\n",
        "        writer.writerow([\"cat\",   cat_metrics[\"MSE\"],   cat_metrics[\"L1\"],   cat_metrics[\"PSNR\"]])\n",
        "        writer.writerow([\"house\", house_metrics[\"MSE\"], house_metrics[\"L1\"], house_metrics[\"PSNR\"]])\n",
        "\n",
        "    return cat_metrics, house_metrics\n"
      ],
      "metadata": {
        "id": "AzWXlrmrbLp6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual code for OOD reconstruction:"
      ],
      "metadata": {
        "id": "-vxi7-qWbqYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Patch: consistent device + consistent scale ([0,1]) for metrics and visuals ---\n",
        "\n",
        "import os, torch, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "# CIFAR-10 normalization\n",
        "_CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465]).view(1,3,1,1)\n",
        "_CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616]).view(1,3,1,1)\n",
        "\n",
        "def _denorm_01(x):  # x in normalized space -> [0,1]\n",
        "    return (x * _CIFAR10_STD + _CIFAR10_MEAN).clamp(0, 1)\n",
        "\n",
        "def _mse_cpu(a, b):  # both assumed CPU, [0,1]\n",
        "    return float(((a - b) ** 2).mean().item())\n",
        "\n",
        "def _l1_cpu(a, b):\n",
        "    return float((a - b).abs().mean().item())\n",
        "\n",
        "def _psnr_cpu(a, b, data_range=1.0):\n",
        "    e = ((a - b) ** 2).mean().item()\n",
        "    if e == 0: return float('inf')\n",
        "    import math\n",
        "    return 20.0 * math.log10(data_range) - 10.0 * math.log10(e)\n",
        "\n",
        "def _save_pair_cpu(original_01, recon_01, path, title_top, title_bottom):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    plt.figure(figsize=(3.2, 6.0))\n",
        "    plt.subplot(2,1,1); plt.imshow(original_01.squeeze(0).permute(1,2,0).numpy()); plt.axis(\"off\"); plt.title(title_top)\n",
        "    plt.subplot(2,1,2); plt.imshow(recon_01.squeeze(0).permute(1,2,0).numpy());    plt.axis(\"off\"); plt.title(title_bottom)\n",
        "    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()\n",
        "\n",
        "\n",
        "def compare_cat_vs_house_reconstruction_FIXED(\n",
        "    vae_model,\n",
        "    normalize_input: bool = True,\n",
        "    decoder_output: str = \"sigmoid\",\n",
        "    out_dir: str = \"plots/vae_compare\"\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Load inputs (possibly normalized if normalize_input=True)\n",
        "    x_cat   = load_cifar10_cat_keras(normalize=normalize_input)   # [1,3,32,32], CPU\n",
        "    x_house = make_synthetic_house(normalize=normalize_input)     # [1,3,32,32], CPU\n",
        "\n",
        "    # Reconstructions (on the model’s device); function returns [0,1]\n",
        "    x_cat_hat   = vae_reconstruct(vae_model, x_cat,   decoder_output=decoder_output).detach().cpu()\n",
        "    x_house_hat = vae_reconstruct(vae_model, x_house, decoder_output=decoder_output).detach().cpu()\n",
        "\n",
        "    # Map originals to [0,1] before comparing, then move to CPU\n",
        "    x_cat_img   = (_denorm_01(x_cat)   if normalize_input else x_cat).detach().cpu()\n",
        "    x_house_img = (_denorm_01(x_house) if normalize_input else x_house).detach().cpu()\n",
        "\n",
        "    # Metrics on CPU in [0,1]\n",
        "    cat_metrics = {\n",
        "        \"MSE\":  _mse_cpu(x_cat_img,   x_cat_hat),\n",
        "        \"L1\":   _l1_cpu(x_cat_img,    x_cat_hat),\n",
        "        \"PSNR\": _psnr_cpu(x_cat_img,  x_cat_hat),\n",
        "    }\n",
        "    house_metrics = {\n",
        "        \"MSE\":  _mse_cpu(x_house_img,   x_house_hat),\n",
        "        \"L1\":   _l1_cpu(x_house_img,    x_house_hat),\n",
        "        \"PSNR\": _psnr_cpu(x_house_img,  x_house_hat),\n",
        "    }\n",
        "\n",
        "    # Visuals\n",
        "    _save_pair_cpu(x_cat_img,   x_cat_hat,   f\"{out_dir}/cat_reconstruction.png\",   \"Original: Cat\",   \"Reconstruction: Cat\")\n",
        "    _save_pair_cpu(x_house_img, x_house_hat, f\"{out_dir}/house_reconstruction.png\", \"Original: House\", \"Reconstruction: House\")\n",
        "\n",
        "    # CSV\n",
        "    import csv\n",
        "    with open(f\"{out_dir}/reconstruction_metrics.csv\", \"w\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"image\", \"MSE\", \"L1\", \"PSNR\"])\n",
        "        w.writerow([\"cat\",   cat_metrics[\"MSE\"],   cat_metrics[\"L1\"],   cat_metrics[\"PSNR\"]])\n",
        "        w.writerow([\"house\", house_metrics[\"MSE\"], house_metrics[\"L1\"], house_metrics[\"PSNR\"]])\n",
        "\n",
        "    return cat_metrics, house_metrics\n",
        "\n",
        "# ---- RUN IT ----\n",
        "cat_metrics, house_metrics = compare_cat_vs_house_reconstruction_FIXED(\n",
        "    vae,                # <-- your VAE variable name\n",
        "    normalize_input=True,     # True if you trained with CIFAR-10 normalization\n",
        "    decoder_output=\"sigmoid\", # \"tanh\" if your decoder outputs [-1,1]\n",
        "    out_dir=f\"{save_path}/plots/vae_compare\"\n",
        ")\n",
        "print(\"Cat metrics:\", cat_metrics)\n",
        "print(\"House metrics:\", house_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTnW_MPqbslt",
        "outputId": "4ef192f2-624a-4a71-9a5b-1856bc6b73cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat metrics: {'MSE': 0.14413614571094513, 'L1': 0.3470536470413208, 'PSNR': 8.41227095422466}\n",
            "House metrics: {'MSE': 0.030931850895285606, 'L1': 0.07367781549692154, 'PSNR': 15.095940919735813}\n"
          ]
        }
      ]
    }
  ]
}